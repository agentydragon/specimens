"""Grading strategy implementations."""

from abc import ABC, abstractmethod
import json
from pathlib import Path
from typing import Any, TypedDict, cast

from adgn.inop.config import OptimizerConfig
from adgn.inop.engine.models import (
    AssistantMessage,
    ComparisonGrading,
    Criterion,
    FileBasedGrading,
    FileInfo,
    FinalOutput,
    GradingConfig,
    GradingContext,
    MessageBasedGrading,
    TaskDefinition,
    ToolCall,
    TrajectoryItem,
)
from adgn.inop.io.yaml_loader import YamlLoader, load_yaml_files
from adgn.inop.prompting.truncation_utils import TruncationManager

# TypedDicts for grading artifacts


class FileBasedArtifacts(TypedDict):
    """Raw artifacts from FileBasedGradingStrategy.collect_artifacts."""

    files: dict[str, str]  # path -> content


class PreparedFileBasedArtifacts(TypedDict):
    """Prepared artifacts from FileBasedGradingStrategy.prepare_for_grader."""

    type: str  # "file_based"
    files: list[dict[str, str]]  # list of {path, content}
    criteria: list[Criterion]


class MessageBasedArtifacts(TypedDict):
    """Raw artifacts from MessageBasedGradingStrategy.collect_artifacts."""

    final_message: str


class PreparedMessageBasedArtifacts(TypedDict):
    """Prepared artifacts from MessageBasedGradingStrategy.prepare_for_grader."""

    type: str  # "message_based"
    message: str
    criteria: list[Criterion]


class ComparisonArtifacts(TypedDict):
    """Raw artifacts from ComparisonGradingStrategy.collect_artifacts."""

    agent_output: str
    reference: str


class PreparedComparisonArtifacts(TypedDict):
    """Prepared artifacts from ComparisonGradingStrategy.prepare_for_grader."""

    type: str  # "comparison"
    agent_output: str
    reference: str
    criteria: list[Criterion]


class GradingStrategy(ABC):
    """Base class for grading strategies."""

    @abstractmethod
    def collect_artifacts(self, context: GradingContext) -> dict[str, Any]:
        """Collect artifacts to be graded from the rollout and environment.

        Args:
            context: Grading context with rollout, task, and environment

        Returns:
            Dictionary of artifacts to grade
        """

    @abstractmethod
    def prepare_for_grader(self, artifacts: dict[str, Any], config: OptimizerConfig) -> dict[str, Any]:
        """Prepare artifacts for the grading model.

        Args:
            artifacts: Raw artifacts from collect_artifacts
            config: Optimizer configuration for truncation settings

        Returns:
            Dictionary ready for grading model
        """

    @abstractmethod
    def get_grading_prompt(self, prepared_artifacts: dict[str, Any], task: TaskDefinition) -> str:
        """Generate the grading prompt for this strategy.

        Args:
            prepared_artifacts: Artifacts prepared by prepare_for_grader
            task: Task being graded

        Returns:
            Prompt string for the grader
        """


class FileBasedGradingStrategy(GradingStrategy):
    """Grade based on files produced by the agent."""

    def __init__(self, criteria: list[Criterion] | None = None):
        self.criteria = criteria if criteria is not None else []

    def collect_artifacts(self, context: GradingContext) -> FileBasedArtifacts:
        """Collect files from environment or rollout.

        Returns:
            Dict with 'files' key mapping file paths to content strings.
        """
        files = {}

        # Try to get files from environment first (container/workspace)
        if context.environment:
            files = context.environment.collect_files()

        # Fall back to files in rollout
        if not files and context.rollout.files:
            files = context.rollout.files

        # Last resort: try to extract from trajectory
        if not files:
            files = self._extract_files_from_trajectory(context.rollout.trajectory)

        return {"files": files}

    def _extract_files_from_trajectory(self, trajectory: list[TrajectoryItem]) -> dict[str, str]:
        """Extract files from Write/Edit tool calls in trajectory."""
        files = {}
        for item in trajectory:
            if isinstance(item, ToolCall) and item.tool_name in ("Write", "Edit", "MultiEdit"):
                # Extract file content from tool call
                args = item.arguments
                if "file_path" in args:
                    path = args["file_path"]
                    if item.tool_name == "Write":
                        content = args.get("content", "")
                    elif item.tool_name == "Edit":
                        content = args.get("new_string", "")
                    else:
                        content = ""  # MultiEdit is more complex
                    files[path] = content
        return files

    def prepare_for_grader(self, artifacts: dict[str, Any], config: OptimizerConfig) -> PreparedFileBasedArtifacts:
        """Truncate files for grading.

        Args:
            artifacts: Raw artifacts dict (expected to be FileBasedArtifacts).
            config: Optimizer configuration for truncation settings.

        Returns:
            Prepared artifacts with truncated files ready for grading model.
        """
        files = artifacts.get("files", {})
        t_mgr = TruncationManager(config)

        # Convert to list format for truncation
        file_list = [{"path": path, "content": content} for path, content in files.items()]

        # Truncate individual files
        truncated_files = []
        for file_info in file_list:
            truncated_content = t_mgr.truncate_text(
                file_info["content"], config.truncation.max_file_size_grading, "... [truncated for grading]"
            )
            truncated_files.append({"path": file_info["path"], "content": truncated_content})

        # Further truncate by total token count
        truncated_union = t_mgr.truncate_files_by_tokens(truncated_files, config.tokens.max_files_tokens)
        # Normalize to list[dict[str, Any]] for downstream JSON
        if truncated_union and isinstance(truncated_union[0], FileInfo):
            tu = cast(list[FileInfo], truncated_union)
            normalized_files: list[dict[str, Any]] = [{"path": fi.path, "content": fi.content} for fi in tu]
        else:
            td = cast(list[dict[str, str]], truncated_union)
            normalized_files = [{"path": d["path"], "content": d["content"]} for d in td]

        return {"type": "file_based", "files": normalized_files, "criteria": self.criteria}

    def get_grading_prompt(self, prepared_artifacts: dict[str, Any], task: TaskDefinition) -> str:
        """Generate file-based grading prompt."""
        files = prepared_artifacts["files"]
        return f"Task: {task.prompt}\n\nFiles:\n{json.dumps(files, indent=2)}"


class MessageBasedGradingStrategy(GradingStrategy):
    """Grade based on final message output."""

    def __init__(self, criteria: list[Criterion] | None = None):
        self.criteria = criteria if criteria is not None else []

    def collect_artifacts(self, context: GradingContext) -> MessageBasedArtifacts:
        """Get final message from trajectory.

        Returns:
            Dict with 'final_message' key containing the agent's final output.
        """
        final_message = ""

        # Look for final output in trajectory
        for item in reversed(context.rollout.trajectory):
            if isinstance(item, FinalOutput):
                final_message = item.text
                break
            if isinstance(item, AssistantMessage):
                # Use last assistant message if no explicit final
                final_message = item.text

        # Fallback to rollout's final_output property
        if not final_message:
            final_message = context.rollout.final_output

        return {"final_message": final_message}

    def prepare_for_grader(self, artifacts: dict[str, Any], config: OptimizerConfig) -> PreparedMessageBasedArtifacts:
        """Truncate message if needed.

        Args:
            artifacts: Raw artifacts dict (expected to be MessageBasedArtifacts).
            config: Optimizer configuration for truncation settings.

        Returns:
            Prepared artifacts with truncated message ready for grading model.
        """
        message = artifacts.get("final_message", "")
        t_mgr = TruncationManager(config)

        # Truncate message to reasonable length
        truncated = t_mgr.truncate_text(
            message,
            config.truncation.max_file_size_grading * 2,  # Allow longer for messages
            "... [truncated]",
        )

        return {"type": "message_based", "message": truncated, "criteria": self.criteria}

    def get_grading_prompt(self, prepared_artifacts: dict[str, Any], task: TaskDefinition) -> str:
        """Generate message-based grading prompt."""
        message = prepared_artifacts["message"]
        return f"Task: {task.prompt}\n\nAgent's Response:\n{message}"


class ComparisonGradingStrategy(GradingStrategy):
    """Grade by comparing output to reference."""

    def __init__(self, reference: str, criteria: list[Criterion] | None = None):
        self.reference = reference
        self.criteria = criteria if criteria is not None else []

    def collect_artifacts(self, context: GradingContext) -> ComparisonArtifacts:
        """Get agent output for comparison.

        Returns:
            Dict with 'agent_output' and 'reference' keys for comparison.
        """
        # Get final message - collect ALL assistant messages for code review
        # since the review might be spread across multiple messages
        all_messages = []
        final_message = ""

        for item in context.rollout.trajectory:
            if isinstance(item, AssistantMessage) and item.text:
                all_messages.append(item.text)
            elif isinstance(item, FinalOutput):
                final_message = item.text
                break

        # Use final output if present, otherwise concatenate all assistant messages
        if final_message:
            agent_output = final_message
        elif all_messages:
            agent_output = "\n\n".join(all_messages)
        else:
            agent_output = context.rollout.final_output

        return {"agent_output": agent_output, "reference": self.reference}

    def prepare_for_grader(self, artifacts: dict[str, Any], config: OptimizerConfig) -> PreparedComparisonArtifacts:
        """Prepare comparison artifacts.

        Args:
            artifacts: Raw artifacts dict (expected to be ComparisonArtifacts).
            config: Optimizer configuration for truncation settings.

        Returns:
            Prepared artifacts with truncated outputs ready for grading model.
        """
        t_mgr = TruncationManager(config)

        agent_output = t_mgr.truncate_text(
            artifacts.get("agent_output", ""), config.truncation.max_file_size_grading, "... [truncated]"
        )

        reference = t_mgr.truncate_text(
            artifacts.get("reference", ""), config.truncation.max_file_size_grading, "... [truncated]"
        )

        return {"type": "comparison", "agent_output": agent_output, "reference": reference, "criteria": self.criteria}

    def get_grading_prompt(self, prepared_artifacts: dict[str, Any], task: TaskDefinition) -> str:
        """Generate comparison grading prompt."""
        agent_output = prepared_artifacts["agent_output"]
        reference = prepared_artifacts["reference"]
        criteria_desc = "\n".join([f"- {c.name}: {c.description}" for c in prepared_artifacts["criteria"]])

        return (
            f"Task: {task.prompt}\n\n"
            f"Agent's Output:\n{agent_output}\n\n"
            f"Reference Output:\n{reference}\n\n"
            f"Grading Criteria:\n{criteria_desc}\n\n"
            f"Compare the agent's output to the reference and grade based on the criteria."
        )


def create_grading_strategy(grading_config: GradingConfig, config_path: Path | None = None) -> GradingStrategy:
    """Factory to create grading strategy from configuration.

    Args:
        grading_config: Grading configuration from task type
        config_path: Base path for loading criteria files

    Returns:
        Appropriate grading strategy instance
    """
    if isinstance(grading_config, FileBasedGrading):
        criteria = grading_config.criteria
        if not criteria and grading_config.criteria_file:
            # Load criteria from file
            if config_path:
                criteria_file = config_path / grading_config.criteria_file
            else:
                criteria_file = Path(grading_config.criteria_file)

            if criteria_file.exists():
                # Load graders YAML via YamlLoader and map to Criterion list
                gl_file: YamlLoader = load_yaml_files(str(criteria_file), str(criteria_file))
                criteria = [Criterion(name=g.id, description=g.description) for g in gl_file.graders_data]
        return FileBasedGradingStrategy(criteria)

    if isinstance(grading_config, MessageBasedGrading):
        criteria = grading_config.criteria
        if not criteria and grading_config.criteria_file:
            if config_path:
                criteria_file = config_path / grading_config.criteria_file
            else:
                criteria_file = Path(grading_config.criteria_file)

            if criteria_file.exists():
                gl2: YamlLoader = load_yaml_files(str(criteria_file), str(criteria_file))
                criteria = [Criterion(name=g.id, description=g.description) for g in gl2.graders_data]
        return MessageBasedGradingStrategy(criteria)

    if isinstance(grading_config, ComparisonGrading):
        if grading_config.reference is None:
            raise ValueError("ComparisonGrading configuration requires a reference output")
        return ComparisonGradingStrategy(reference=grading_config.reference, criteria=grading_config.criteria)

    raise ValueError(f"Unknown grading config type: {type(grading_config)}")
