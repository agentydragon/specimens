apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: kube-prometheus-stack
  namespace: monitoring
spec:
  interval: 30m
  timeout: 10m # Prometheus stack upgrades can be slow
  chart:
    spec:
      chart: kube-prometheus-stack
      version: "69.5.0"
      sourceRef:
        kind: HelmRepository
        name: prometheus-community
        namespace: flux-system
      interval: 12h
  valuesFrom:
    # Inject complete OIDC configuration from Vault (via ExternalSecret)
    # Terraform writes the full config to Vault, eliminating duplication
    # Secret structured with full nesting to avoid targetPath escaping issues
    - kind: Secret
      name: grafana-oidc-config
      valuesKey: values.yaml
  values:
    # Global settings
    defaultRules:
      create: true
      rules:
        alertmanager: true
        etcd: false # Talos doesn't expose etcd metrics
        configReloaders: true
        general: true
        k8s: true
        kubeApiserverAvailability: true
        kubeApiserverSlos: true
        kubelet: true
        kubeProxy: true
        kubePrometheusGeneral: true
        kubePrometheusNodeRecording: true
        kubernetesApps: true
        kubernetesResources: true
        kubernetesStorage: true
        kubernetesSystem: true
        kubeScheduler: true
        kubeStateMetrics: true
        network: true
        node: true
        nodeExporterAlerting: true
        nodeExporterRecording: true
        prometheus: true
        prometheusOperator: true

    # Alertmanager configuration
    alertmanager:
      enabled: true
      ingress:
        enabled: false # Can enable later with authentication
      alertmanagerSpec:
        storage:
          volumeClaimTemplate:
            spec:
              storageClassName: proxmox-csi-retain
              accessModes: ["ReadWriteOnce"]
              resources:
                requests:
                  storage: 5Gi
        resources:
          requests:
            cpu: 10m
            memory: 64Mi
          limits:
            cpu: 100m
            memory: 128Mi
        retention: 120h

    # Grafana configuration
    grafana:
      enabled: true
      # Admin password managed via ExternalSecret
      admin:
        existingSecret: grafana-admin-password
        passwordKey: admin-password
      defaultDashboardsEnabled: true
      defaultDashboardsTimezone: UTC

      ingress:
        enabled: true
        ingressClassName: nginx
        annotations:
          cert-manager.io/cluster-issuer: letsencrypt-prod
          external-dns.alpha.kubernetes.io/hostname: grafana.test-cluster.agentydragon.com
        hosts:
          - grafana.test-cluster.agentydragon.com
        tls:
          - secretName: grafana-tls
            hosts:
              - grafana.test-cluster.agentydragon.com

      # CRITICAL: Use Recreate strategy for RWO volumes
      # Grafana uses RWO persistent volume for dashboards, plugins, and session data (10Gi).
      # RollingUpdate + RWO volumes = deadlock when pods fail during initialization.
      # Init:Error pods hold RWO volumes, preventing new pods from attaching.
      # This has caused production deadlocks requiring manual intervention (3rd occurrence).
      # See docs/RWO_VOLUME_DEPLOYMENT_STRATEGY.md for complete explanation.
      # Brief downtime during updates is acceptable vs manual deadlock recovery.
      deploymentStrategy:
        type: Recreate

      persistence:
        enabled: true
        storageClassName: proxmox-csi-retain
        accessModes:
          - ReadWriteOnce
        size: 10Gi

      resources:
        requests:
          cpu: 50m
          memory: 128Mi
        limits:
          cpu: 200m
          memory: 256Mi

      # Enable useful plugins (no Angular plugins)
      plugins:
        - grafana-clock-panel

      # Datasources - use default Prometheus datasource from chart
      # Add Loki datasource for log aggregation
      additionalDataSources:
        - name: Loki
          type: loki
          access: proxy
          url: http://loki-stack.loki.svc.cluster.local:3100
          jsonData:
            maxLines: 1000
          isDefault: false
          editable: true

      # Authentik SSO configuration injected via valuesFrom from grafana-oidc-config secret
      # Complete config stored in Vault by Terraform (single source of truth)
      grafana.ini:
        server:
          root_url: https://grafana.test-cluster.agentydragon.com
        # auth.generic_oauth populated automatically via valuesFrom

    # Prometheus Operator configuration
    prometheusOperator:
      enabled: true
      resources:
        requests:
          cpu: 50m
          memory: 128Mi
        limits:
          cpu: 200m
          memory: 256Mi

      # Admission webhook resource limits
      admissionWebhooks:
        patch:
          resources:
            requests:
              cpu: 10m
              memory: 32Mi
            limits:
              cpu: 100m
              memory: 64Mi

    # Prometheus configuration
    prometheus:
      enabled: true
      ingress:
        enabled: false # Can enable later with authentication

      prometheusSpec:
        # Retention settings
        retention: 7d
        retentionSize: "15GB"

        # Storage configuration
        storageSpec:
          volumeClaimTemplate:
            spec:
              storageClassName: proxmox-csi-retain
              accessModes: ["ReadWriteOnce"]
              resources:
                requests:
                  storage: 20Gi

        # Resource limits for small cluster
        resources:
          requests:
            cpu: 200m
            memory: 1Gi
          limits:
            cpu: 1000m
            memory: 2Gi

        # Scrape interval
        scrapeInterval: 30s
        evaluationInterval: 30s

        # Service monitor selector - select all service monitors
        serviceMonitorSelectorNilUsesHelmValues: false
        podMonitorSelectorNilUsesHelmValues: false
        ruleSelectorNilUsesHelmValues: false

        # Security context
        securityContext:
          runAsNonRoot: true
          runAsUser: 1000
          fsGroup: 2000

    # Node exporter configuration
    nodeExporter:
      enabled: true
      resources:
        requests:
          cpu: 10m
          memory: 32Mi
        limits:
          cpu: 100m
          memory: 64Mi

    # Kube-state-metrics configuration
    kube-state-metrics:
      resources:
        requests:
          cpu: 10m
          memory: 64Mi
        limits:
          cpu: 100m
          memory: 128Mi

    # Disable components we don't need
    kubeApiServer:
      enabled: true
    kubelet:
      enabled: true
    kubeControllerManager:
      enabled: true
    coreDns:
      enabled: true
    kubeEtcd:
      enabled: false # Talos doesn't expose etcd metrics
    kubeScheduler:
      enabled: true
    kubeProxy:
      enabled: true
