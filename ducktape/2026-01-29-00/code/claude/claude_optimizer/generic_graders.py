#!/usr/bin/env python3
"""
Generic parametric graders for evaluating Claude agent behavior.

Two main types:
1. CodeGrader - Evaluates final code files generated by Claude in agent directory
2. ActionSequenceGrader - Evaluates the sequence of actions/tools Claude used
"""

import json
from dataclasses import dataclass
from pathlib import Path
from typing import Any

from openai import OpenAI
from pydantic import BaseModel, ConfigDict

from claude.claude_optimizer.scoresheet import GradeResult, GraderScoresheet, TestCase


class Action(BaseModel):
    """Single action in agent rollout sequence"""

    tool: str | None = None
    type: str | None = None
    description: str | None = None

    model_config = ConfigDict(extra="allow")  # Allow additional fields


@dataclass
class BehavioralRequirement:
    """A specific behavioral requirement to evaluate against."""

    id: str
    name: str
    description: str
    evaluation_criteria: str
    analysis_prompt_template: str
    function_schema: dict[str, Any]


@dataclass
class AgentRollout:
    """Information about a Claude agent's execution in a directory."""

    agent_dir: Path
    claude_md_content: str
    task_prompt: str
    generated_files: list[Path]  # Files Claude created in the directory
    action_sequence: list[Action]  # Tool calls and actions Claude made
    final_output: str  # Claude's final response text
    success: bool  # Whether the rollout completed successfully


class CodeGrader(GraderScoresheet):
    """Generic grader that evaluates final code files generated by Claude agents."""

    def __init__(self, requirement: BehavioralRequirement, openai_model: str):
        """
        Initialize code grader.

        Args:
            requirement: The behavioral requirement to evaluate against
            openai_model: OpenAI model to use for analysis (no default - must be explicit)
        """
        super().__init__(requirement.name)
        self.requirement = requirement
        self.openai_client = OpenAI()
        self.openai_model = openai_model

    @property
    def description(self) -> str:
        return self.requirement.description

    async def generate_test_cases(self) -> list[TestCase]:
        """Generate test cases based on the behavioral requirement."""
        # For now, return a single test case that represents the requirement
        # Later, this could be made more sophisticated
        return [
            TestCase(
                id=self.requirement.id,
                name=self.requirement.name,
                prompt=f"Evaluate code compliance with: {self.requirement.description}",
                description=self.requirement.evaluation_criteria,
                expected_behavior=self.requirement.description,
            )
        ]

    async def grade_agent_rollout(self, rollout: AgentRollout) -> GradeResult:
        """
        Grade a Claude agent rollout based on the behavioral requirement.

        Args:
            rollout: Complete information about the agent's execution

        Returns:
            GradeResult with analysis of code compliance
        """
        try:
            # Collect all generated code from the agent directory
            all_code = ""
            code_files = []

            for file_path in rollout.generated_files:
                if file_path.suffix in {".py", ".js", ".ts", ".java", ".cpp", ".c", ".go", ".rs"}:
                    try:
                        code_content = file_path.read_text()
                        all_code += f"\n\n# File: {file_path.name}\n{code_content}"
                        code_files.append(file_path)
                    except (FileNotFoundError, PermissionError, UnicodeDecodeError) as e:
                        # Log but continue with other files
                        all_code += f"\n\n# File: {file_path.name} (Error reading: {e})"

            if not all_code.strip():
                return GradeResult(
                    test_case_id=self.requirement.id,
                    test_case_name=self.requirement.name,
                    passed=False,
                    score=0.0,
                    feedback="No code files found in agent directory",
                    generated_code=None,
                    analysis_details={
                        "error": "no_code_files",
                        "agent_dir": str(rollout.agent_dir),
                        "files_found": [str(f) for f in rollout.generated_files],
                    },
                )

            # Use OpenAI function calling to analyze the code
            analysis_messages = [
                {
                    "role": "user",
                    "content": self.requirement.analysis_prompt_template.format(
                        code=all_code,
                        task_prompt=rollout.task_prompt,
                        claude_md_content=rollout.claude_md_content,
                        requirement_description=self.requirement.description,
                        evaluation_criteria=self.requirement.evaluation_criteria,
                    ),
                }
            ]

            openai_response = self.openai_client.responses.create(  # type: ignore[call-overload]
                model=self.openai_model,
                input=analysis_messages,
                tools=[
                    {
                        "type": "function",
                        "name": self.requirement.function_schema["name"],
                        **self.requirement.function_schema,
                    }
                ],
                tool_choice={"type": "function", "name": self.requirement.function_schema["name"]},
            )

            # Extract analysis results from function call
            analysis_data = None
            for output_item in openai_response.output:
                if output_item.type == "function_call" and output_item.name == self.requirement.function_schema["name"]:
                    analysis_data = json.loads(output_item.arguments)
                    break

            if not analysis_data:
                return GradeResult(
                    test_case_id=self.requirement.id,
                    test_case_name=self.requirement.name,
                    passed=False,
                    score=0.0,
                    feedback="Analysis failed - no function call response",
                    generated_code=all_code,
                    analysis_details={"error": "no_analysis_response"},
                )

            self._log_interaction(
                test_case_name=self.requirement.name,
                request_type="code_analysis",
                claude_request={"messages": analysis_messages, "tools": [self.requirement.function_schema]},
                claude_response={"function_call": analysis_data},
                success=True,
            )

            # Extract standardized fields from analysis_data
            # Different requirements may have different field names, so we use common patterns
            has_problems = analysis_data.get("has_problems", analysis_data.get("has_violations", True))
            problems = analysis_data.get("problems", analysis_data.get("violations", []))
            assessment = analysis_data.get("assessment", "Analysis completed")
            score = analysis_data.get("score", 0.0)

            # Grade based on analysis
            passed = not has_problems and score >= 0.8

            if passed:
                feedback = f"✅ Complies with {self.requirement.name}: {assessment}"
            else:
                feedback = f"❌ Violates {self.requirement.name}:\n"
                feedback += f"Assessment: {assessment}\n"
                if problems:
                    feedback += "Issues found:\n"
                    for i, problem in enumerate(problems[:3], 1):  # Limit to 3 problems
                        # Handle different problem formats
                        if isinstance(problem, dict):
                            pattern = problem.get("pattern", problem.get("issue", "Unknown"))
                            reason = problem.get("reason", problem.get("description", "No reason"))
                            feedback += f"{i}. {pattern}: {reason}\n"
                        else:
                            feedback += f"{i}. {problem}\n"

            return GradeResult(
                test_case_id=self.requirement.id,
                test_case_name=self.requirement.name,
                passed=passed,
                score=score,
                feedback=feedback,
                generated_code=all_code,
                analysis_details={
                    **analysis_data,
                    "agent_dir": str(rollout.agent_dir),
                    "code_files": [str(f) for f in code_files],
                    "openai_model_used": self.openai_model,
                },
            )

        except (json.JSONDecodeError, KeyError) as e:
            error_msg = f"Code grading failed - response parsing error: {e}"
            self._log_interaction(
                test_case_name=self.requirement.name,
                request_type="error",
                claude_request={},
                claude_response={},
                success=False,
                error_message=error_msg,
            )
            return GradeResult(
                test_case_id=self.requirement.id,
                test_case_name=self.requirement.name,
                passed=False,
                score=0.0,
                feedback=error_msg,
                generated_code=None,
                analysis_details={"error": str(e)},
            )
        except (FileNotFoundError, PermissionError, UnicodeDecodeError) as e:
            error_msg = f"Code grading failed - file system error: {e}"
            self._log_interaction(
                test_case_name=self.requirement.name,
                request_type="error",
                claude_request={},
                claude_response={},
                success=False,
                error_message=error_msg,
            )
            return GradeResult(
                test_case_id=self.requirement.id,
                test_case_name=self.requirement.name,
                passed=False,
                score=0.0,
                feedback=error_msg,
                generated_code=None,
                analysis_details={"error": str(e)},
            )


class ActionSequenceGrader(GraderScoresheet):
    """Generic grader that evaluates the sequence of actions/tools Claude used."""

    def __init__(self, requirement: BehavioralRequirement, openai_model: str):
        """
        Initialize action sequence grader.

        Args:
            requirement: The behavioral requirement to evaluate against
            openai_model: OpenAI model to use for analysis (no default - must be explicit)
        """
        super().__init__(requirement.name)
        self.requirement = requirement
        self.openai_client = OpenAI()
        self.openai_model = openai_model

    @property
    def description(self) -> str:
        return self.requirement.description

    async def generate_test_cases(self) -> list[TestCase]:
        """Generate test cases based on the behavioral requirement."""
        return [
            TestCase(
                id=self.requirement.id,
                name=self.requirement.name,
                prompt=f"Evaluate action sequence compliance with: {self.requirement.description}",
                description=self.requirement.evaluation_criteria,
                expected_behavior=self.requirement.description,
            )
        ]

    async def grade_agent_rollout(self, rollout: AgentRollout) -> GradeResult:
        """
        Grade a Claude agent rollout based on action sequence analysis.

        Args:
            rollout: Complete information about the agent's execution

        Returns:
            GradeResult with analysis of action sequence compliance
        """
        # Format action sequence for analysis
        action_summary = ""
        for i, action in enumerate(rollout.action_sequence, 1):
            action_type = action.tool or action.type or "unknown"
            description = action.description or str(action.model_dump())[:100]
            action_summary += f"{i}. {action_type}: {description}\n"

        if not action_summary.strip():
            return GradeResult(
                test_case_id=self.requirement.id,
                test_case_name=self.requirement.name,
                passed=False,
                score=0.0,
                feedback="No action sequence found",
                generated_code=None,
                analysis_details={"error": "no_action_sequence", "agent_dir": str(rollout.agent_dir)},
            )

        # Use OpenAI function calling to analyze the action sequence
        analysis_messages = [
            {
                "role": "user",
                "content": self.requirement.analysis_prompt_template.format(
                    action_sequence=action_summary,
                    task_prompt=rollout.task_prompt,
                    claude_md_content=rollout.claude_md_content,
                    final_output=rollout.final_output,
                    requirement_description=self.requirement.description,
                    evaluation_criteria=self.requirement.evaluation_criteria,
                ),
            }
        ]

        openai_response = self.openai_client.responses.create(  # type: ignore[call-overload]
            model=self.openai_model,
            input=analysis_messages,
            tools=[
                {
                    "type": "function",
                    "name": self.requirement.function_schema["name"],
                    **self.requirement.function_schema,
                }
            ],
            tool_choice={"type": "function", "name": self.requirement.function_schema["name"]},
        )

        # Extract and process analysis results (similar to CodeGrader)
        analysis_data = None
        for output_item in openai_response.output:
            if output_item.type == "function_call" and output_item.name == self.requirement.function_schema["name"]:
                analysis_data = json.loads(output_item.arguments)
                break

        if not analysis_data:
            raise RuntimeError("Analysis failed - no function call response")

        self._log_interaction(
            test_case_name=self.requirement.name,
            request_type="action_analysis",
            claude_request={"messages": analysis_messages, "tools": [self.requirement.function_schema]},
            claude_response={"function_call": analysis_data},
            success=True,
        )

        # Process results (same logic as CodeGrader)
        has_problems = analysis_data.get("has_problems", analysis_data.get("has_violations", True))
        problems = analysis_data.get("problems", analysis_data.get("violations", []))
        assessment = analysis_data.get("assessment", "Analysis completed")
        score = analysis_data.get("score", 0.0)

        passed = not has_problems and score >= 0.8

        if passed:
            feedback = f"✅ Action sequence complies with {self.requirement.name}: {assessment}"
        else:
            feedback = f"❌ Action sequence violates {self.requirement.name}:\n"
            feedback += f"Assessment: {assessment}\n"
            if problems:
                feedback += "Issues found:\n"
                for i, problem in enumerate(problems[:3], 1):
                    if isinstance(problem, dict):
                        issue = problem.get("action", problem.get("issue", "Unknown"))
                        reason = problem.get("reason", problem.get("description", "No reason"))
                        feedback += f"{i}. {issue}: {reason}\n"
                    else:
                        feedback += f"{i}. {problem}\n"

        return GradeResult(
            test_case_id=self.requirement.id,
            test_case_name=self.requirement.name,
            passed=passed,
            score=score,
            feedback=feedback,
            generated_code=action_summary,  # Store action sequence as "code" for consistency
            analysis_details={
                **analysis_data,
                "agent_dir": str(rollout.agent_dir),
                "action_count": len(rollout.action_sequence),
                "openai_model_used": self.openai_model,
            },
        )
