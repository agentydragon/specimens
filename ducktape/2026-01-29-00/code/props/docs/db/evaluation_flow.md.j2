# Evaluation Flow: Critic → Grader → Metrics

This document describes the end-to-end evaluation pipeline for prompt optimization.

## Pipeline Overview

```
Critic Run                    Grader Run                 Metrics
───────────                   ──────────                 ───────
Source code ─┐
             ├─> Critique ────> Match to TPs/FPs ────> Recall
System prompt┘   (reported_issues)  (grading_edges)        (aggregate views)
```

{{ describe_relation("snapshots") }}
{{ describe_relation("snapshot_files") }}

## 1. Critic Run

**What the critic sees:**
- Source code mounted at `/workspace` (read-only)
- System prompt with task description
- Database access for writing issues

**What the critic DOES NOT see:**
- Ground truth issues (TPs/FPs)
- Expected output or "answers"
- Grader feedback or metrics

**Critic's task:** Review code, report issues, and call submit when done.

## 2. Grader Run

**Input:**
- Critique from critic (query `reported_issues`, `reported_issue_occurrences`)
- Ground truth from snapshot (query `true_positives`, `true_positive_occurrences`, `false_positives`, `false_positive_occurrences`)

**Task:** Fill in a bipartite graph connecting critique issues to GT occurrences.

Every (critique_issue, matchable_gt_occurrence) pair needs an edge with credit:
- **1.0** = perfect match (issue accurately describes occurrence)
- **0.0** = no match (issue doesn't describe occurrence)
- **Partial** = issue partially captures the occurrence

**Matchability:** An occurrence is matchable from a critique issue if:
- No file restriction is specified (`graders_match_only_if_reported_on IS NULL`), OR
- The critique touches files that overlap with the occurrence's file scope

Use `props grader-agent list pending` to see missing edges. Grading is complete when no pending edges remain.

**Output:** `grading_edges` table populated with edges between issues and GT occurrences.

## 3. Metrics Computation

**Per-run:** Recall = sum(found_credit) / count(occurrences)

**Aggregate:** Query views like `recall_by_definition_split_kind`

## Recall Views

{{ describe_relation("recall_by_definition_split_kind") }}
{{ describe_relation("recall_by_definition_example") }}

### Weighting

Cross-run recall is weighted by occurrence, not by example:
- Formula: `SUM(AVG(found_credit) per occurrence) / COUNT(occurrences)`
- Examples with more TP occurrences naturally weight more
- This measures "total issues found / total issues in dataset"

### Failure Handling

When a critic exceeds max_turns or context limits:
- No valid critique is produced
- Counts as zero-recall for that example

### Pareto Frontier (Best Definitions per Example)

{{ describe_relation("pareto_frontier_by_example") }}

Use this to find:
- Which definitions achieve best recall on each example
- Difficult examples where even the best definition struggles
- Definition specialization patterns

```sql
-- Find hardest examples (lowest best recall)
SELECT snapshot_slug, files_hash, best_mean_credit
FROM pareto_frontier_by_example
WHERE split = 'train'
ORDER BY best_mean_credit ASC
LIMIT 10;

-- Which definitions win most often on validation?
SELECT unnest(winning_critic_image_digests) AS def_id, COUNT(*) AS wins
FROM pareto_frontier_by_example
WHERE split = 'valid'
GROUP BY def_id ORDER BY wins DESC;
```

## Your Optimization Mode
{% if config.target_metric == "whole-repo" %}
You are running in **whole-repo mode**.

**Terminal metric:** Whole-snapshot recall on VALID split.

**Validation access:**
- Examples table: RLS-blocked (no filenames visible)
- Can only run whole-snapshot evaluations
- Ground truth: Hidden
- Execution traces: Hidden

**Query validation metrics:**

{{ describe_relation("validation_recall_by_definition") }}
{% elif config.target_metric == "targeted" %}
You are running in **targeted mode**.

**Terminal metric:** Per-file and whole-snapshot recall on VALID split.

**Validation access:**
- Examples table: Accessible (filenames visible)
- Can run both per-file and whole-snapshot evaluations
- Ground truth: Still hidden
- Execution traces: Still hidden

**Query validation metrics:**
```sql
-- Aggregate by definition
SELECT (credit_stats).mean AS recall, n_examples,
       (credit_stats).ucb95 AS ucb, (credit_stats).lcb95 AS lcb
FROM recall_by_definition_split_kind
WHERE critic_image_digest = '<def_id>' AND split = 'valid';

-- Per-example breakdown
SELECT snapshot_slug, files_hash, example_kind,
       (credit_stats).mean AS recall,
       (credit_stats).lcb95 AS lcb,
       (credit_stats).ucb95 AS ucb
FROM recall_by_definition_example
WHERE critic_image_digest = '<def_id>'
  AND split = 'valid'
ORDER BY (credit_stats).mean DESC;
```

**IMPORTANT:** Always check `n_examples >= 5` before trusting metrics (small samples = high variance).
{% else %}
**Note:** No target_metric specified. See documentation for whole-repo vs targeted modes.
{% endif %}

## Data Access Patterns

### Training Split (Full Access)

```sql
-- Get examples
SELECT * FROM examples e
JOIN snapshots s ON e.snapshot_slug = s.slug
WHERE s.split = 'train';

-- Access ground truth
SELECT * FROM true_positives WHERE snapshot_slug = '<slug>';

-- Read execution traces
SELECT * FROM events WHERE agent_run_id = '<run_id>' ORDER BY sequence_num;
```

### Validation Split (Restricted)

{% if config.target_metric == "whole-repo" %}
```sql
-- Query aggregate metrics only (examples table is RLS-blocked)
SELECT * FROM get_validation_full_snapshot_aggregates()
WHERE critic_image_digest = '<def_id>';

-- CANNOT query examples table directly
-- CANNOT inspect ground truth or execution traces
```
{% elif config.target_metric == "targeted" %}
```sql
-- Can see example filenames
SELECT * FROM examples e
JOIN snapshots s ON e.snapshot_slug = s.slug
WHERE s.split = 'valid';

-- Query metrics via views
SELECT (credit_stats).mean AS recall
FROM recall_by_definition_split_kind
WHERE critic_image_digest = '<def_id>' AND split = 'valid';

-- CANNOT inspect ground truth or execution traces
```
{% endif %}
